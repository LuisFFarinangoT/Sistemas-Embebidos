double kerneldensity(double *samples, double obs, size_t n);
* (Silverman 1986, pg 48 eq 3.31).  This is the default
* bandwith estimator for the R 'density' function.  */
double nrd0(double x[], const int N)
{
gsl_sort(x, 1, N);
double hi = gsl_stats_sd(x, 1, N);
double iqr =
gsl_stats_quantile_from_sorted_data (x,1, N,0.75) -
gsl_stats_quantile_from_sorted_data (x,1, N,0.25);
double lo = GSL_MIN(hi, iqr/1.34);
double bw = 0.9 * lo * pow(N,-0.2);
return(bw);
}
/* kernels for kernel density estimates */
double gauss_kernel(double x)
{
return exp(-(gsl_pow_2(x)/2))/(M_SQRT2*sqrt(M_PI));
}
double kerneldensity(double *samples, double obs, size_t n)
{
size_t i;
double h = GSL_MAX(nrd0(samples, n), 1e-6);
double prob = 0;
for(i=0; i < n; i++)
{
prob += gauss_kernel( (samples[i] - obs)/h)/(n*h);
}
return prob;
}
clc
end
clc
clclclclcclc
clc
cclc
clclc
end
int
int 64
clc
end
endsWith()
power
power.anova.test(15)
setwd("D:/utn/7mo/Sistemas Embebidos/Deberes/Analsis de Datos en R/Curva ROC")
library(knitr)
library(ROCR)
library(e1071)
library(caTools)
library(nnet)
library(scales)
set.seed(222)
#Cargara la base de datos al entorno de R
datos=read.csv("datos.csv",header=TRUE,sep=",")
datos$PH=ifelse(datos$PH>7,1,0)#
datos$PH=as.factor(datos$PH)
str(datos)
indice=sample(1:nrow(datos),size = 0.8*nrow(datos))
training=datos[indice,]
test=datos[-indice,]
library(randomForest)
#MODELo random forest
rf_model=randomForest(PH~.,data =training)
rf_prediction=predict(rf_model,test,type='vote')
#modelo de regresion lineal
#lr_model=glm(PH~.,data=training,family = "binomial")
#lr_pred=prediction(lr_model,tes,type="response")
#curvas roc
library(pROC)
ROC_rf = roc(test$PH,rf_prediction[,2])
#ROC_lr <- roc(test$PH, lr_prediction)
plot(ROC_rf)
install.packages(c("BH", "caret", "cli", "fansi", "pROC", "recipes", "stringi"))
library(knitr)
library(ROCR)
library(e1071)
library(caTools)
library(nnet)
library(scales)
set.seed(222)
#Cargara la base de datos al entorno de R
datos=read.csv("datos.csv",header=TRUE,sep=",")
datos$PH=ifelse(datos$PH>7,1,0)#
datos$PH=as.factor(datos$PH)
str(datos)
indice=sample(1:nrow(datos),size = 0.8*nrow(datos))
training=datos[indice,]
test=datos[-indice,]
library(randomForest)
#MODELo random forest
rf_model=randomForest(PH~.,data =training)
rf_prediction=predict(rf_model,test,type='vote')
#modelo de regresion lineal
#lr_model=glm(PH~.,data=training,family = "binomial")
#lr_pred=prediction(lr_model,tes,type="response")
#curvas roc
library(pROC)
ROC_rf = roc(test$PH,rf_prediction[,2])
#ROC_lr <- roc(test$PH, lr_prediction)
plot(ROC_rf)
myurl <- "https://raw.githubusercontent.com/spitakiss/Data607_Pres1/master/FullCoverage.csv"
full.cov = read.csv(myurl,header=TRUE)
full.cov$men <- factor(full.cov$men)
full.cov$urban <- factor(full.cov$urban)
full.cov$private <- factor(full.cov$private)
full.cov$y <- factor(full.cov$y)
kable(head(full.cov))
kable(summary(full.cov))
attach(full.cov)
marital = relevel(marital,ref="S")
# Fit to Logistic regression
FullcovModel = glm(y~men+urban+private+factor(marital)+age+seniority,family=binomial(link=logit))
# Model Output
summary(FullcovModel)
# Calculate Pseudo R-2
FullcovModelRed <- glm(y~1, family=binomial(link=logit))
1-(logLik(FullcovModel))/(logLik(FullcovModelRed))
rf <- randomForest(y~men+urban+private+factor(marital)+age+seniority,data=full.cov)
# Model output
print(rf)
# Calculate sensitivity and false positive measures for logit model
fity_ypos <- FullcovModel$fitted[y == 1]
fity_yneg <- FullcovModel$fitted[y == 0]
sort_fity <- sort(FullcovModel$fitted.values)
sens <- 0
spec_c <- 0
for (i in length(sort_fity):1){
sens <- c(sens, mean(fity_ypos >= sort_fity[i]))
spec_c <- c(spec_c, mean(fity_yneg >= sort_fity[i]))
}
# Calculate sensitivity and false positive measure for random forest model
fity_ypos2 <- as.numeric(rf$pred[y == 1]) - 1
fity_yneg2 <- as.numeric(rf$pred[y == 0]) - 1
sort_fity2 <- as.numeric(sort(rf$pred)) - 1
sens2 <- 0
spec_c2 <- 0
for (i in length(sort_fity2):1){
sens2 <- (c(sens2, mean(fity_ypos2 >= sort_fity2[i])))
spec_c2 <- (c(spec_c2, mean(fity_yneg2 >= sort_fity2[i])))
}
# plot ROC curves
plot(spec_c, sens, xlim = c(0, 1), ylim = c(0, 1), type = "l",
xlab = "false positive rate", ylab = "true positive rate", col = 'blue')
abline(0, 1, col= "black")
lines(spec_c2, sens2, col='green')
legend("topleft", legend = c("logit",
"random forest") ,
pch = 15, bty = 'n', col = c("blue","green"))
ROC_rf = roc(test$PH,rf_prediction[,2])
library(knitr)
library(ROCR)
library(e1071)
library(caTools)
library(nnet)
library(scales)
set.seed(222)
#Cargara la base de datos al entorno de R
datos=read.csv("datos.csv",header=TRUE,sep=",")
datos$PH=ifelse(datos$PH>7,1,0)#
datos$PH=as.factor(datos$PH)
str(datos)
indice=sample(1:nrow(datos),size = 0.8*nrow(datos))
training=datos[indice,]
test=datos[-indice,]
library(randomForest)
#MODELo random forest
rf_model=randomForest(PH~.,data =training)
rf_prediction=predict(rf_model,test,type='vote')
#modelo de regresion lineal
#lr_model=glm(PH~.,data=training,family = "binomial")
#lr_pred=prediction(lr_model,tes,type="response")
#curvas roc
library(pROC)
ROC_rf = roc(test$PH,rf_prediction[,2])
#ROC_lr <- roc(test$PH, lr_prediction)
plot(ROC_rf)
ROC_rf = roc(training$PH,rf_prediction[,2])
View(test)
datos$PH=ifelse(datos$PH>7.5,1,0)#
datos$PH=ifelse(datos$PH>7.49,1,0)#
datos$PH=as.factor(datos$PH)
indice=sample(1:nrow(datos),size = 0.8*nrow(datos))
training=datos[indice,]
test=datos[-indice,]
library(randomForest)
#MODELo random forest
rf_model=randomForest(PH~.,data =training)
rf_prediction=predict(rf_model,test,type='vote')
#MODELo random forest
rf_model=randomForest(PH~.,data =training)
training=datos[indice,]
test=datos[-indice,]
library(randomForest)
#MODELo random forest
rf_model=randomForest(PH~.,data =training)
rf_prediction=predict(rf_model,test,type='vote')
#MODELo random forest
rf_model=randomForest(PH~.,data =training)
ROC_rf = roc(test$PH,rf_prediction[,2])
#Cargara la base de datos al entorno de R
datos=read.csv("datos.csv",header=TRUE,sep=",")
datos$Temperatura=ifelse(datos$Temperatura>15,1,0)#
datos$Temperatura=as.factor(datos$Temperatura)
indice=sample(1:nrow(datos),size = 0.8*nrow(datos))
training=datos[indice,]
test=datos[-indice,]
#MODELo random forest
rf_model=randomForest(Temperatura~.,data =training)
rf_prediction=predict(rf_model,test,type='vote')
ROC_rf = roc(test$Temperatura,rf_prediction[,2])
#ROC_lr <- roc(test$PH, lr_prediction)
plot(ROC_rf)
library(knitr)
library(ROCR)
library(e1071)
library(caTools)
library(nnet)
library(scales)
set.seed(222)
#Cargara la base de datos al entorno de R
datos=read.csv("datos.csv",header=TRUE,sep=",")
datos$Temperatura=ifelse(datos$Temperatura>15,1,0)#
datos$Temperatura=as.factor(datos$Temperatura)
str(datos)
indice=sample(1:nrow(datos),size = 0.8*nrow(datos))
training=datos[indice,]
test=datos[-indice,]
library(randomForest)
#MODELo random forest
rf_model=randomForest(Temperatura~.,data =training)
rf_prediction=predict(rf_model,test,type='vote')
#modelo de regresion lineal
#lr_model=glm(PH~.,data=training,family = "binomial")
#lr_pred=prediction(lr_model,tes,type="response")
#curvas roc
library(pROC)
ROC_rf = roc(test$Temperatura,rf_prediction[,2])
#ROC_lr <- roc(test$PH, lr_prediction)
plot(ROC_rf)
